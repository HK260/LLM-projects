{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f69e6-9cd8-465b-9fe0-9ad9bea7f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0508a56-f83c-4996-92ac-ee6bfdd81aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd15d3d7-66a6-4044-961f-7fb942c07021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1088, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 123\n",
      "Document types found: {'products', 'contracts', 'company', 'employees'}\n"
     ]
    }
   ],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "\n",
    "def add_metadata(doc, doc_type):\n",
    "    doc.metadata[\"doc_type\"] = doc_type\n",
    "    return doc\n",
    "\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total number of chunks: {len(chunks)}\")\n",
    "print(f\"Document types found: {set(doc.metadata['doc_type'] for doc in documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f7601b-ef2d-442a-93d8-1a6b3cf83692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 123 documents\n"
     ]
    }
   ],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "DB_NAME = \"vector_db\"\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Delete if already exists\n",
    "\n",
    "if os.path.exists(DB_NAME):\n",
    "    Chroma(persist_directory=DB_NAME, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=DB_NAME)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e6d4a2f-ac20-4015-829b-e4ae834a96c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run a quick test - should return a list of documents = 4\n",
    "question = \"contract with Belvedere Insurance\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09d8bce2-2ac9-46d4-a035-6da9657baa1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id='e9bf36ed-7831-4b6c-a22e-7f5e12353016', metadata={'source': 'knowledge-base\\\\contracts\\\\Contract with Belvedere Insurance for Markellm.md', 'doc_type': 'contracts'}, page_content='# Contract with Belvedere Insurance for Markellm\\n\\n## Terms\\nThis Contract (\"Agreement\") is made and entered into as of [Date] by and between Insurellm, Inc., a corporation registered in the United States, (\"Provider\") and Belvedere Insurance, (\"Client\"). \\n\\n1. **Service Commencement**: The services described herein will commence on [Start Date].\\n2. **Contract Duration**: This Agreement shall remain in effect for a period of 1 year from the Commencement Date, unless terminated earlier in accordance with the termination clause of this Agreement.\\n3. **Fees**: Client agrees to pay a Basic Listing Fee of $199/month for accessing the Markellm platform along with a performance-based pricing of $25 per lead generated.\\n4. **Payment Terms**: Payments shall be made monthly, in advance, with invoices issued on the 1st of each month, payable within 15 days of receipt.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "364c4a53-b71b-40da-8663-d826d8a02efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_23816\\1192425351.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# create a new Chat with Ollama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "MODEL = \"llama3.2:latest\"\n",
    "llm = ChatOllama(temperature=0.7, model=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "342a8ccf-94ae-472a-a8c6-61fa97f8aa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text is a sample contract between Insurellm, Inc. (the provider) and Belvedere Insurance (the client). The contract outlines the terms and conditions of their partnership, which involves using the Markellm platform for customer acquisition.\n",
      "\n",
      "Here's a breakdown of the key points:\n",
      "\n",
      "**Contract Overview**\n",
      "\n",
      "* The contract commences on a specific start date and remains in effect for 1 year, unless terminated earlier.\n",
      "* Belvedere Insurance agrees to pay a monthly Basic Listing Fee of $199 and a performance-based pricing of $25 per lead generated.\n",
      "\n",
      "**Payment Terms**\n",
      "\n",
      "* Payments are due monthly, in advance, with invoices issued on the 1st of each month. Clients must pay within 15 days of receiving the invoice.\n",
      "\n",
      "**Support and Training**\n",
      "\n",
      "* Technical support is available from 9 AM to 7 PM EST, Monday through Friday via email and phone.\n",
      "* Insurellm agrees to respond to all support queries within 24 business hours, with emergency support prioritized.\n",
      "* A comprehensive training session will be offered for Belvedere Insurance's staff upon beginning the service.\n",
      "\n",
      "**Features**\n",
      "\n",
      "* The contract includes access to Markellm's AI-powered matching, real-time quotes, data insights, and a analytics dashboard.\n",
      "* Optional premium features and analytics can be purchased for an additional $9.99/month.\n",
      "\n",
      "**Termination**\n",
      "\n",
      "* Either party may terminate the contract with 30 days' written notice.\n",
      "\n",
      "This contract outlines the terms of the partnership between Insurellm, Inc. and Belvedere Insurance, which involves using the Markellm platform for customer acquisition. The contract includes payment terms, support arrangements, and access to various features that enhance the client's ability to acquire customers.\n"
     ]
    }
   ],
   "source": [
    "# Let's try a simple question\n",
    "\n",
    "query = \"Explain contract with Belvedere Insurance\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dafa326b-9372-43b2-bf48-b19eae721694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the  LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aac6b96-d073-4725-b704-0c6f9dc0e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping that in a function\n",
    "\n",
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3034103-6b1a-4642-b0c3-6a6fa8f571db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the rating\n",
    "user_rating = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "824ff6cc-b2d9-45c8-b4f0-58b87eb1d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_rating(value): # we store the rating from the user\n",
    "    global user_rating\n",
    "    user_rating.append(value)\n",
    "    return f\"Thanks! Your rating ({value}/5) has been recorded.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3173ef9e-9866-4f14-aa61-96e437d38fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### Chat with the assistant and then rate your experience\")\n",
    "\n",
    "    # ChatInterface\n",
    "    chatbot = gr.ChatInterface(fn=chat, type=\"messages\")\n",
    "\n",
    "    # Rating section\n",
    "    gr.Markdown(\"#### Rate the assistant (1-5)\")\n",
    "    rating_slider = gr.Slider(minimum=1, maximum=5, step=1, label=\"Your Rating\")\n",
    "    submit_button = gr.Button(\"Submit Rating\")\n",
    "    output_text = gr.Textbox(label=\"Feedback Confirmation\")\n",
    "\n",
    "    # Link rating button to function\n",
    "    submit_button.click(fn=store_rating, inputs=rating_slider, outputs=output_text)\n",
    "\n",
    "demo.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84a03f3f-f44b-475e-9164-0dce2ede388c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c12b3b2-1e28-4576-a101-1e910762ab3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llms]",
   "language": "python",
   "name": "conda-env-llms-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

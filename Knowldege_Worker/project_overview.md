```markdown
# ğŸ§  Expert Knowledge Worker â€“ QA Assistant for Insurellm

A Retrieval-Augmented Generation (RAG) based Question Answering assistant built for _Insurellm_, an insurance tech company. This system uses a local LLM to accurately respond to user queries based on a custom markdown document knowledge base â€” all implemented in a Jupyter notebook for flexibility and rapid experimentation.

---

## ğŸš€ Features

- ğŸ” Contextual QA from 31 markdown documents representing internal insurance knowledge
- ğŸ§  Runs locally with **LLaMA 3 (via Ollama)** â€” zero API cost and full data privacy
- ğŸ“¦ Uses **ChromaDB** for document indexing and retrieval
- ğŸ“˜ Fully implemented in a Jupyter Notebook environment
- â±ï¸ Average query response time: ~5 seconds
- â­ Users can rate answers from 1â€“5; feedback stored for future evaluation

---

## ğŸ§° Tech Stack

- **LLM:** LLaMA 3 (via Ollama)
- **Framework:** LangChain
- **Vector Store:** ChromaDB
- **Embeddings:** Ollama Embeddings
- **Documents:** Markdown files (synthetic insurance content)
- **Environment:** Jupyter Notebook

---

## âš™ï¸ How to Run

1. Launch the notebook (`expert_knowledge_worker.ipynb`), run all cells sequentially.
2. Input a question when prompted and receive an answer with a rating option.

---

## ğŸ“ Evaluation

- â±ï¸ **Average Response Time:** \~5 seconds per query (local inference)
- ğŸ“š **Document Base:** 31 markdown files
- â­ **User Ratings:** Collected (1â€“5 scale) per query to support future accuracy tracking
```
